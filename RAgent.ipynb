{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503aa5e5-f520-4dc8-9632-b62a479cedda",
   "metadata": {},
   "source": [
    "### MAKE SURE TO ADD DOCUMENTATION TO YOUR NOTEBOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fadcf9b-2dfb-404a-a498-c9daab6e3186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7879\n",
      "* Running on public URL: https://ef716888a47967aba9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ef716888a47967aba9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gradio as gr\n",
    "import sys\n",
    "import io\n",
    "import subprocess\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "_ = load_dotenv()\n",
    "model = \"gpt-4o\"\n",
    "client = OpenAI()\n",
    "\n",
    "# Agent logs and code history context\n",
    "agent_logs = []\n",
    "past_code_snippets = []\n",
    "\n",
    "# Detect package usage for agent routing\n",
    "def detect_pkgs(text):\n",
    "    text = text.lower()\n",
    "    if \"lmer\" in text or \"lme4\" in text or \"lmertest\" in text:\n",
    "        return 'lmer'\n",
    "    elif \"ggplot\" in text or \"aes\" in text or \"geom_\" in text:\n",
    "        return 'ggplot'\n",
    "    elif \"survey\" in text:\n",
    "        return 'survey'\n",
    "    elif \"lm(\" in text or \"glm(\" in text or \"regression\" in text:\n",
    "        return 'lm_glm'\n",
    "    else:\n",
    "        return 'general'\n",
    "\n",
    "# Define prompt based on specialty\n",
    "def system_prompt(specialty):\n",
    "    base = '''You are a helpful assistant that specializes in statistical methods, specifically in R code. \n",
    "    This includes knowledge on the content in standard packages like tidyverse and dplyer as well as more advanced packages like lmerTest.\n",
    "    Your job is to help beginners to experts and students to professors alike in choosing accurate statistical methods and packages for their projects. \n",
    "    If the user is unsure what piece of code to write or statistical method to use, you will ask for context on their project, including dataset information and outcome expectations, as well as desired significance, applications, and relationships of variables.\n",
    "    Any suggestions you give should be on a case-by-case basis, meaning you will suggest code for one test at a time. In any case scenario, you MUST stick to one statistical test at a time. \n",
    "    Be assertive in confirming if a single piece of code works before seeing if further results or plots should be made. \n",
    "    Ask for a copy of the output if you are unsure that the user has failed in conducting the test accurately. \n",
    "    If the user asks for multiple pieces of code, you will start with the first request and ensure that there are no bugs and a useful outcome is obtained before moving to the next requested code. \n",
    "    If a user is asking for explanations of the outcomes for a statistical method they have already coded, please make sure you understand all of the input variables as well as any fine tuning that was done before creating a confident response of what the outcome means in terms of the project. \n",
    "    You will also ask if a writeup should be made in any particular format (bullet points, paragraph, outline) to illuminate the meaning behind the outcome. \n",
    "    Let's walk through an example. If a user sends the outputs of a regression model lm(), you should confirm what the user is looking to find and/or the variables included in the experiment before selecting a top few findings in the regression results to highlight in the context of the experiment. \n",
    "    That means creating a brief response with a very specific and confident main point centered around one or two of the regression findings. If the user asks for more explanations, you may give it but your initial response should be concise and confident. \n",
    "    When the user asks you to explain an output, you should focus about being direct and assertive towards what the numerical outputs in the linear model explain in regards to the variables tested. \n",
    "    This means immediately pointing out significance, fixed effects, or potential issues in the model before explaining anything regarding definitions, or how a linear regression actually works. \n",
    "    Your focus should be on brevity with smart suggestions and intelligent insights. Try your best to be witty but useful.\n",
    "    If the user does something incorrectly, you should be brief and responsive, highlighting the immediate fix before asking clarification questions if you are unsure what the exact issue is.\n",
    "    Your questions and responses should be assertive. Do not explain anything that was not asked to be explain and try to keep up with the expected knowledge of the user before explaining introductory or advanced concepts. \n",
    "    You may ask an introductory question about the users experience level with the requested analysis or code or dataset if you are unsure whether or not you are explaining concepts that are already understood. To make it clear, a user could ask for assistance with a piece of code but still be completely knowledgeable about the concept or topic.'''\n",
    "    \n",
    "    if specialty == 'lmer':\n",
    "        base = base.replace(\"specifically in R code\", \"specifically in R code, with an emphasis on mixed models and the lme4/lmerTest packages\")\n",
    "    elif specialty == 'ggplot':\n",
    "        base = base.replace(\"specifically in R code\", \"specifically in R code, with a specialization in data visualization using ggplot2\")\n",
    "    elif specialty == 'survey':\n",
    "        base = base.replace(\"specifically in R code\", \"specifically in R code, with a focus on complex survey design and the survey package\")\n",
    "    elif specialty == 'lm_glm':\n",
    "        base = base.replace(\"specifically in R code\", \"specifically in R code, with a particular emphasis on linear and generalized linear models\")\n",
    "    return base\n",
    "\n",
    "# Extract R code suggestions\n",
    "def extract_code(text):\n",
    "    matches = re.findall(r\"```r(.*?)```\", text, re.DOTALL | re.IGNORECASE)\n",
    "    if matches:\n",
    "        return [m.strip() for m in matches]\n",
    "    matches = re.findall(r\"(library\\(.*?\\)|ggplot\\(.*?\\)|data\\(.*?\\)|.+?%>%)\", text)\n",
    "    return [m.strip() for m in matches]\n",
    "\n",
    "# Confirm code addition\n",
    "def confirm_code(current_code, proposed_code):\n",
    "    inserted_code = proposed_code.strip()\n",
    "    past_code_snippets.append(inserted_code)\n",
    "    new_code = current_code + \"\\n\\n# Suggested by RAgent\\n\" + inserted_code\n",
    "    agent_logs.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"action\": \"confirm_code_addition\",\n",
    "        \"inserted_code\": inserted_code\n",
    "    })\n",
    "    return new_code, gr.update(visible=False)\n",
    "\n",
    "# ðŸ”¥ LLM-based diff agent\n",
    "def llm_generate_diff(old_code, new_code):\n",
    "    diff_prompt = f\"\"\"You are an assistant that detects and summarizes the important changes between two versions of R code.\n",
    "\n",
    "Compare the following two R scripts:\n",
    "\n",
    "OLD CODE:\n",
    "{old_code}\n",
    "\n",
    "NEW CODE:\n",
    "{new_code}\n",
    "\n",
    "First, list the MAJOR differences clearly.\n",
    "Then, suggest a compact R code diff if possible.\n",
    "\n",
    "Be precise, and focus on meaning changes, not just formatting.\n",
    "If the two scripts are completely different, just explain the high-level changes instead of showing line-by-line diff.\n",
    "\"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You compare R code versions and detect meaningful changes.\"},\n",
    "                  {\"role\": \"user\", \"content\": diff_prompt}],\n",
    "        n=1,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Apply confirmed code\n",
    "def apply_diff(current_code, proposed_code):\n",
    "    updated_code = proposed_code.strip()\n",
    "    past_code_snippets.append(updated_code)\n",
    "    agent_logs.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"action\": \"apply_diff\",\n",
    "        \"inserted_code\": updated_code\n",
    "    })\n",
    "    return updated_code, gr.update(visible=False)\n",
    "\n",
    "# RAgent code\n",
    "def ask_ragent(user_msg, history, current_code, console_output):\n",
    "    old_code = current_code.strip() if current_code else \"\"\n",
    "    #context_code = \"\\n\\n\".join(past_code_snippets[-3:]) + \"\\n\\n\" + current_code\n",
    "    specialty = detect_pkgs(old_code + \" \" + user_msg)\n",
    "    agent_prompt = system_prompt(specialty)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": agent_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Here's my current code:\\n{old_code}\\nConsole output:\\n{console_output}\\n{user_msg}\"}\n",
    "    ]\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        n=1,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    reply = completion.choices[0].message.content\n",
    "\n",
    "    diff_summary = \"\"\n",
    "    proposed = \"\"\n",
    "    code_blocks = extract_code(reply)\n",
    "    if code_blocks:\n",
    "        proposed = code_blocks[0]\n",
    "        diff_summary = llm_generate_diff(old_code, proposed)\n",
    "        reply += f\"\\n\\n---\\nHereâ€™s the diff between your current code and the proposed changes:\\n{diff_summary}\"\n",
    "\n",
    "    show_confirm = bool(proposed)\n",
    "    \n",
    "    agent_logs.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"specialty\": specialty,\n",
    "        \"user_msg\": user_msg,\n",
    "        \"proposed_code\": proposed,\n",
    "        \"diff_summary\": diff_summary,\n",
    "        \"code_detected\": bool(proposed)\n",
    "    })\n",
    "    history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    return \"\", history, proposed, gr.update(visible=show_confirm)\n",
    "\n",
    "def run_r(code):\n",
    "    with open(\"temp_code.R\", \"w\") as f:\n",
    "        f.write(code + \"\\n\\nggsave('plot.png', width=6, height=4)\")\n",
    "    try:\n",
    "        result = subprocess.run([\"Rscript\", \"temp_code.R\"], capture_output=True, text=True, timeout=10)\n",
    "        output = result.stdout + \"\\n\" + result.stderr\n",
    "        plot_exists = os.path.exists(\"plot.png\")\n",
    "    except Exception as e:\n",
    "        output = f\"Error running R script: {e}\"\n",
    "        plot_exists = False\n",
    "    return output, code, output, \"plot.png\" if plot_exists else None\n",
    "\n",
    "# Export agent logs\n",
    "def export_logs():\n",
    "    if not agent_logs:\n",
    "        return None\n",
    "    df = pd.DataFrame(agent_logs)\n",
    "    df.to_csv(\"agent_logs.csv\", index=False)\n",
    "    return \"agent_logs.csv\"\n",
    "\n",
    "# gradio\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"RAgent\")\n",
    "    code_history = gr.State(\"\")\n",
    "    console_history = gr.State(\"\")\n",
    "    proposed_code = gr.State(\"\")\n",
    "    chat_state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            code_input = gr.Code(language=\"r\", label=\"Enter R code\")\n",
    "            run_button = gr.Button(\"Run Code\")\n",
    "            code_output = gr.Textbox(label=\"Console Output\", lines=10)\n",
    "            plot_output = gr.Image(type=\"filepath\", label=\"Plot Preview\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(label=\"RAgent Assistant\", type=\"messages\", value=[\n",
    "                {\"role\": \"assistant\", \"content\": \"Hello! How can I assist your coding today?\"}])\n",
    "            chat_input = gr.Textbox(label=\"Ask a question or request help\")\n",
    "            send_button = gr.Button(\"Send Message\")\n",
    "            confirm_button = gr.Button(\"Add Code\", visible=False)\n",
    "            replace_button = gr.Button(\"Replace Lines\", visible=False)\n",
    "\n",
    "    run_button.click(run_r, inputs=code_input, outputs=[code_output, code_history, console_history, plot_output])\n",
    "    send_button.click(ask_ragent, inputs=[chat_input, chat_state, code_history, console_history], outputs=[chat_input, chatbot, proposed_code, confirm_button])\n",
    "    confirm_button.click(apply_diff, inputs=[code_input, proposed_code], outputs=[code_input, confirm_button])\n",
    "    #replace_button.click(confirm_replacement, inputs=[code_input], outputs=[code_input, replace_button])\n",
    "\n",
    "    with gr.Row():\n",
    "        log_export = gr.Button(\"Export Logs\")\n",
    "        log_output = gr.File(label=\"Download Logs\")\n",
    "        log_export.click(fn=export_logs, inputs=[], outputs=log_output)\n",
    "\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8755d9b9-d8d3-4d7b-b321-a48db5000fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
