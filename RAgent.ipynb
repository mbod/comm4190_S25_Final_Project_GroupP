{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fadcf9b-2dfb-404a-a498-c9daab6e3186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://817cb41f871f824fc0.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://817cb41f871f824fc0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gradio as gr\n",
    "import sys\n",
    "import io\n",
    "import subprocess\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "_ = load_dotenv()\n",
    "model = \"gpt-4o\"\n",
    "client = OpenAI()\n",
    "\n",
    "# Agent logs and code history context\n",
    "agent_logs = []\n",
    "past_code_snippets = []\n",
    "\n",
    "# Detect package usage for agent routing\n",
    "def detect_pkgs(text):\n",
    "    text = text.lower()\n",
    "    if \"lmer\" in text or \"lme4\" in text or \"lmertest\" in text:\n",
    "        return 'lmer'\n",
    "    elif \"ggplot\" in text or \"aes\" in text or \"geom_\" in text:\n",
    "        return 'ggplot'\n",
    "    elif \"survey\" in text:\n",
    "        return 'survey'\n",
    "    elif \"lm(\" in text or \"glm(\" in text or \"regression\" in text:\n",
    "        return 'lm_glm'\n",
    "    else:\n",
    "        return 'general'\n",
    "\n",
    "# Define prompt based on specialty\n",
    "def system_prompt(specialty):\n",
    "    base = '''You are a helpful assistant that specializes in statistical methods, specifically in R code. \n",
    "    This includes knowledge on the content in standard packages like tidyverse and dplyer as well as more advanced packages like lmerTest.\n",
    "    Your job is to help beginners to experts and students to professors alike in choosing accurate statistical methods and packages for their projects. \n",
    "    If the user is unsure what piece of code to write or statistical method to use, you will ask for context on their project, including dataset information and outcome expectations, as well as desired significance, applications, and relationships of variables.\n",
    "    Any suggestions you give should be on a case-by-case basis, meaning you will suggest code for one test at a time. In any case scenario, you MUST stick to one statistical test at a time. \n",
    "    Be assertive in confirming if a single piece of code works before seeing if further results or plots should be made. \n",
    "    Ask for a copy of the output if you are unsure that the user has failed in conducting the test accurately. \n",
    "    If the user asks for multiple pieces of code, you will start with the first request and ensure that there are no bugs and a useful outcome is obtained before moving to the next requested code. \n",
    "    If a user is asking for explanations of the outcomes for a statistical method they have already coded, please make sure you understand all of the input variables as well as any fine tuning that was done before creating a confident response of what the outcome means in terms of the project. \n",
    "    You will also ask if a writeup should be made in any particular format (bullet points, paragraph, outline) to illuminate the meaning behind the outcome. \n",
    "    Let's walk through an example. If a user sends the outputs of a regression model lm(), you should confirm what the user is looking to find and/or the variables included in the experiment before selecting a top few findings in the regression results to highlight in the context of the experiment. \n",
    "    That means creating a brief response with a very specific and confident main point centered around one or two of the regression findings. If the user asks for more explanations, you may give it but your initial response should be concise and confident. \n",
    "    When the user asks you to explain an output, you should focus about being direct and assertive towards what the numerical outputs in the linear model explain in regards to the variables tested. \n",
    "    This means immediately pointing out significance, fixed effects, or potential issues in the model before explaining anything regarding definitions, or how a linear regression actually works. \n",
    "    Your focus should be on brevity with smart suggestions and intelligent insights. Try your best to be witty but useful.\n",
    "    If the user does something incorrectly, you should be brief and responsive, highlighting the immediate fix before asking clarification questions if you are unsure what the exact issue is.\n",
    "    Your questions and responses should be assertive. Do not explain anything that was not asked to be explain and try to keep up with the expected knowledge of the user before explaining introductory or advanced concepts. \n",
    "    You may ask an introductory question about the users experience level with the requested analysis or code or dataset if you are unsure whether or not you are explaining concepts that are already understood. To make it clear, a user could ask for assistance with a piece of code but still be completely knowledgeable about the concept or topic.'''\n",
    "    \n",
    "    if specialty == 'lmer':\n",
    "        base = base.replace(\"specifically in R code\", \"specifically in R code, with an emphasis on mixed models and the lme4/lmerTest packages\")\n",
    "    elif specialty == 'ggplot':\n",
    "        base = base.replace(\"specifically in R code\", \"specifically in R code, with a specialization in data visualization using ggplot2\")\n",
    "    elif specialty == 'survey':\n",
    "        base = base.replace(\"specifically in R code\", \"specifically in R code, with a focus on complex survey design and the survey package\")\n",
    "    elif specialty == 'lm_glm':\n",
    "        base = base.replace(\"specifically in R code\", \"specifically in R code, with a particular emphasis on linear and generalized linear models\")\n",
    "    return base\n",
    "\n",
    "def extract_code(text):\n",
    "    matches = re.findall(r\"r(.*?)\", text, re.DOTALL)\n",
    "    return [code.strip() for code in matches]\n",
    "\n",
    "# Propose code edits with confirm logic\n",
    "def replace_lines(code, replacement_text, start_line, end_line):\n",
    "    lines = code.splitlines()\n",
    "    replacement_lines = replacement_text.strip().splitlines()\n",
    "    updated_lines = lines[:start_line-1] + replacement_lines + lines[end_line:]\n",
    "    return \"\\n\".join(updated_lines)\n",
    "\n",
    "def confirm_code(current_code, proposed_code):\n",
    "    inserted_code = proposed_code.strip()\n",
    "    past_code_snippets.append(inserted_code)\n",
    "    new_code = current_code + \"\\n\\n# Suggested by RAgent\\n\" + inserted_code\n",
    "    agent_logs.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"action\": \"confirm_code_addition\",\n",
    "        \"inserted_code\": inserted_code\n",
    "    })\n",
    "    return new_code, gr.update(visible=False)\n",
    "\n",
    "def confirm_replacement(current_code, replacement_code, start_line, end_line):\n",
    "    updated_code = replace_lines(current_code, replacement_code, int(start_line), int(end_line))\n",
    "    agent_logs.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"action\": \"replace_lines\",\n",
    "        \"lines\": f\"{start_line}-{end_line}\",\n",
    "        \"replacement_code\": replacement_code\n",
    "    })\n",
    "    return updated_code, gr.update(visible=False)\n",
    "\n",
    "# Agent code\n",
    "\n",
    "def ask_ragent(user_msg, history, current_code, console_output):\n",
    "    context_code = \"\\n\\n\".join(past_code_snippets[-3:]) + \"\\n\\n\" + current_code\n",
    "    specialty = detect_pkgs(context_code + \" \" + user_msg)\n",
    "    agent_prompt = system_prompt(specialty)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": agent_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Here's my current code:\\n{context_code}\\nConsole output:\\n{console_output}\\n{user_msg}\"}\n",
    "    ]\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        n=1,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    reply = completion.choices[0].message.content\n",
    "\n",
    "    proposed = \"\"\n",
    "    show_confirm = False\n",
    "    code_blocks = extract_code(reply)\n",
    "    if code_blocks:\n",
    "        proposed = code_blocks[0]\n",
    "        reply += \"\\n\\n**Would you like to add or replace code?**\"\n",
    "        show_confirm = True\n",
    "\n",
    "    agent_logs.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"specialty\": specialty,\n",
    "        \"user_msg\": user_msg,\n",
    "        \"proposed_code\": proposed,\n",
    "        \"code_detected\": bool(proposed)\n",
    "    })\n",
    "    history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    return \"\", history, proposed, gr.update(visible=show_confirm)\n",
    "\n",
    "def run_r(code):\n",
    "    with open(\"temp_code.R\", \"w\") as f:\n",
    "        f.write(code + \"\\n\\nggsave('plot.png', width=6, height=4)\")\n",
    "    try:\n",
    "        result = subprocess.run([\"Rscript\", \"temp_code.R\"], capture_output=True, text=True, timeout=10)\n",
    "        output = result.stdout + \"\\n\" + result.stderr\n",
    "        plot_exists = os.path.exists(\"plot.png\")\n",
    "    except Exception as e:\n",
    "        output = f\"Error running R script: {e}\"\n",
    "        plot_exists = False\n",
    "    return output, code, output, \"plot.png\" if plot_exists else None\n",
    "\n",
    "# Export agent logs\n",
    "def export_logs():\n",
    "    if not agent_logs:\n",
    "        return None\n",
    "    df = pd.DataFrame(agent_logs)\n",
    "    df.to_csv(\"agent_logs.csv\", index=False)\n",
    "    return \"agent_logs.csv\"\n",
    "\n",
    "# gradio\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"RAgent\")\n",
    "    code_history = gr.State(\"\")\n",
    "    console_history = gr.State(\"\")\n",
    "    proposed_code = gr.State(\"\")\n",
    "    chat_state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            code_input = gr.Code(language=\"r\", label=\"Enter R code\")\n",
    "            run_button = gr.Button(\"Run Code\")\n",
    "            code_output = gr.Textbox(label=\"Console Output\", lines=10)\n",
    "            plot_output = gr.Image(type=\"filepath\", label=\"Plot Preview\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            chatbot = gr.Chatbot(label=\"RAgent Assistant\", type=\"messages\", value=[\n",
    "                {\"role\": \"assistant\", \"content\": \"Hello! How can I assist your coding today?\"}])\n",
    "            chat_input = gr.Textbox(label=\"Ask a question or request help\")\n",
    "            send_button = gr.Button(\"Send Message\")\n",
    "            confirm_button = gr.Button(\"‚úÖ Add Code\", visible=False)\n",
    "            replace_button = gr.Button(\"‚ôªÔ∏è Replace Lines\", visible=False)\n",
    "\n",
    "    run_button.click(run_r, inputs=code_input, outputs=[code_output, code_history, console_history, plot_output])\n",
    "    send_button.click(ask_ragent, inputs=[chat_input, chat_state, code_history, console_history], outputs=[chat_input, chatbot, proposed_code, confirm_button])\n",
    "    confirm_button.click(confirm_code, inputs=[code_input, proposed_code], outputs=[code_input, confirm_button])\n",
    "    #replace_button.click(confirm_replacement, inputs=[code_input], outputs=[code_input, replace_button])\n",
    "\n",
    "    with gr.Row():\n",
    "        log_export = gr.Button(\"üìä Export Logs\")\n",
    "        log_output = gr.File(label=\"Download Logs\")\n",
    "        log_export.click(fn=export_logs, inputs=[], outputs=log_output)\n",
    "\n",
    "app.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
